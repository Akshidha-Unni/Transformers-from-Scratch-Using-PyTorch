{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e770e50",
      "metadata": {},
      "source": [
        "### **Transformers from Scratch using PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43925aec",
      "metadata": {},
      "source": [
        "**Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2f5896d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (2.9.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: setuptools in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 26.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "389b60c4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: lightning in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (2.6.1)\n",
            "Requirement already satisfied: PyYAML<8.0,>5.4 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2028.0,>=2022.5.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<2028.0,>=2022.5.0->lightning) (2025.5.1)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from lightning) (0.15.2)\n",
            "Requirement already satisfied: packaging<27.0,>=23.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from lightning) (24.2)\n",
            "Requirement already satisfied: torch<4.0,>=2.1.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from lightning) (2.9.0)\n",
            "Requirement already satisfied: torchmetrics<3.0,>0.7.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from lightning) (1.8.2)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from lightning) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>4.5.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from lightning) (4.14.0)\n",
            "Requirement already satisfied: pytorch-lightning in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from lightning) (2.6.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<2028.0,>=2022.5.0->lightning) (3.13.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (80.9.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch<4.0,>=2.1.0->lightning) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch<4.0,>=2.1.0->lightning) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch<4.0,>=2.1.0->lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
            "Requirement already satisfied: numpy>1.20.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from torchmetrics<3.0,>0.7.0->lightning) (2.2.6)\n",
            "Requirement already satisfied: colorama in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from tqdm<6.0,>=4.57.0->lightning) (0.4.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\akshi\\appdata\\roaming\\python\\python313\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2028.0,>=2022.5.0->lightning) (3.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 26.0\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3fdd6408",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch #To create tensors \n",
        "import torch.nn as nn  #for Module, Linear, Embedding\n",
        "import torch.nn.functional as F #for softmax fn\n",
        "\n",
        "from torch.optim import Adam #to fit the NN to data with backpropagation\n",
        "from torch.utils.data import TensorDataset, DataLoader #tools to create a large scale Transformer Network with lots of training data\n",
        "\n",
        "import lightning as L #for automatic code optimization and scaling in the cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "793b6642",
      "metadata": {},
      "source": [
        "**Create training dataset for the transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "56d8a789",
      "metadata": {},
      "outputs": [],
      "source": [
        "token_to_id = {'what':0, 'is':1, 'CNN':2, 'Great':3, '<EOS>':4}\n",
        "# These are the overall tokens in the prompt, we map them to numbers because PyTorch word embedding fn nn.Embedding() only accepts numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cc187121",
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a reverse dict to map from id back to token\n",
        "id_to_token = dict(map(reversed, token_to_id.items()))\n",
        "# These two dicts token_to_id and id_to_token makes it easier to format input to transformer and interpret output from Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a824f0",
      "metadata": {},
      "source": [
        "**Process the prompt**\n",
        "\n",
        "It should start with \"What\" -> outputs \"is\",\"is\" -> outputs \"CNN\", \"CNN\" -> outputs \"<EOS>\", <\"<EOS>\" -> outputs \"Great\", \"Great\" -> outputs \"<EOS>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9091d4ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Input tensor \n",
        "inputs = torch.tensor([[token_to_id[\"what\"],\n",
        "                      token_to_id[\"is\"],\n",
        "                      token_to_id[\"CNN\"],\n",
        "                      token_to_id[\"<EOS>\"],\n",
        "                      token_to_id[\"Great\"]], #For 1st prompt \"What is CNN\"\n",
        "\n",
        "                      [token_to_id[\"CNN\"],\n",
        "                      token_to_id[\"is\"],\n",
        "                      token_to_id[\"what\"],\n",
        "                      token_to_id[\"<EOS>\"],\n",
        "                      token_to_id[\"Great\"]]]) #For 2nd prompt \"CNN is What\"\n",
        "#Labels - output from the decoder \n",
        "labels = torch.tensor([[token_to_id[\"is\"],\n",
        "                       token_to_id[\"CNN\"],\n",
        "                       token_to_id[\"<EOS>\"],\n",
        "                       token_to_id[\"Great\"],\n",
        "                       token_to_id[\"<EOS>\"]], #For 1st prompt \"What is CNN\"\n",
        "\n",
        "                       [token_to_id[\"is\"],\n",
        "                       token_to_id[\"CNN\"],\n",
        "                       token_to_id[\"<EOS>\"],\n",
        "                       token_to_id[\"Great\"],\n",
        "                       token_to_id[\"<EOS>\"]]]) #For 2nd prompt \"CNN is What\"\n",
        "\n",
        "# Pass the dataset to DataLoader to create dataloader\n",
        "dataset = TensorDataset(inputs, labels)\n",
        "dataloader = DataLoader(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82be86bc",
      "metadata": {},
      "source": [
        "**Position Embedding**\n",
        "\n",
        "PE (pos, 2i) = sin(pos/10000^(2i/d model)\n",
        "\n",
        "PE (pos, 2i+1) = cos(pos/10000^(2i/d model) +1 in 2i+1 just \n",
        "means cosine comes after sine.\n",
        "where pos -> position of the token\n",
        "i -> each embedding position\n",
        "\n",
        "Rather than computing the formula values for each tokens we pre-compute the y-axis values and store them in a matrix it make it much easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4da74ab3",
      "metadata": {},
      "outputs": [],
      "source": [
        "#To precompute and add positional encoding values to tokens\n",
        "class PositionEncoding (nn.Module): #inherits from nn.module\n",
        "    def __init__(self, d_model=2, max_len=6): #d_model - number of word embeddings per token\n",
        "        #max_len is the max no of tokens our transformer can process (input and output combined)\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model) #create a matrix of positional encoding values and fill it with zeros\n",
        "        # pe = max_len x d_model\n",
        "        \n",
        "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1) #arange() to create a seq of nos between 0 & max_len\n",
        "        #these numbers are float, unsqueeze(1) turns seq of nos into column matrix.\n",
        "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
        "        #step=2 results same seq no that we get if we multiplied i by 2.\n",
        "        \n",
        "        # div_term is the divisor in the formula above which divides the pos.\n",
        "        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe) #to move pe to GPU if we use one.\n",
        "\n",
        "    def forward(self, word_embeddings): #takes word embeddings and adds positional embeddings\n",
        "        return word_embeddings + self.pe[:word_embeddings.size(0), :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effb2348",
      "metadata": {},
      "source": [
        "**Masked Attention**\n",
        "\n",
        "We need to calculate : Query , Key and Value for all tokens.\n",
        "If we have a Matrix with WE+PE for each token (in each row) and we have weights to calculate query as matrix, then we can perform simple matrix multiplication. We can do the same for Key and Value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4a63f6f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model=2): # here d_model is needed in order to know the dimension of the weight matrices.\n",
        "\n",
        "        super().__init__() \n",
        "\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False) #to create the Weight Mx for Q, use Linear()\n",
        "        # Linear() here not just creates the weights will also do the math in further steps\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        #to keep track of rows and columns, useful to train input either sequentially or in batches\n",
        "        self.row_dim = 0 \n",
        "        self.col_dim = 1\n",
        "\n",
        "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
        "        # here encoding_for_q/k/v allows encodings to come from different sources (like in the case of encoder-decoder transformers)\n",
        "        q = self.W_q(encodings_for_q) #we pass the encodings for each token\n",
        "        k = self.W_k(encodings_for_k)\n",
        "        v = self.W_v(encodings_for_v)\n",
        "\n",
        "        sims = torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim)) #calculates the similarity between Q and K\n",
        "\n",
        "        scaled_sims = sims / torch.tensor(k.size(self.col_dim)**0.5) #scale sims by sq root of no of values used in each key\n",
        "        #this scaling is a standard practice since the original transformers (Not required - only helps when model is large)\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9) #adding a mask if we are using one\n",
        "        # mask is optional its like masking the tokens to just see the previous tokens , there are other types of masks as well.\n",
        "        # masked_fill only fills the value in the cells of the matrix whose attention values should be ignored \n",
        "\n",
        "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim) #pass the similarities to softmax\n",
        "\n",
        "        attention_scores = torch.matmul(attention_percents, v) #scale the V matrix with softmax outputs\n",
        "\n",
        "        return attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb615c1",
      "metadata": {},
      "source": [
        "**Building a decoder-only transformer** (Word Embedding + Position Embedding + Attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0e204446",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderOnlyTransformer(L.LightningModule): #inherit from lightning class\n",
        "\n",
        "    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model) #word embedding\n",
        "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len) #position embedding\n",
        "        self.self_attention = Attention(d_model=d_model) #masked self-attention\n",
        "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
        "\n",
        "        self.loss = nn.CrossEntropyLoss() #to quantify how well the model performs, this will apply the softmax fn itself\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "\n",
        "        word_embeddings = self.we(token_ids) #convert tokens to word embeddings\n",
        "        position_encoded = self.pe(word_embeddings)\n",
        "        \n",
        "        #defining the mask that prevents the tokens to look at succeeding tokens when we calculate Attention\n",
        "        mask = torch.tril(torch.ones((token_ids.size(dim=0),token_ids.size(dim=0))))\n",
        "        #first it creates a matrix will all 1s with symmetrical with dimensions of token size\n",
        "        #this is passed to tril (lower-triangle), leaves 1s in lower triangle as it self and turns everything else to 0 (upper triangle values)\n",
        "        mask = mask == 0 #to convert 0->True , 1->False in the matrix\n",
        "        \n",
        "        #calculating the attention\n",
        "        self_attention_values = self.self_attention(position_encoded, position_encoded, position_encoded, mask=mask)\n",
        "        #position_encoded passed thrice for Q, K and V.\n",
        "\n",
        "        #Adding residual connections\n",
        "        residual_connection_values = position_encoded + self_attention_values\n",
        "\n",
        "        #pass to the FC layer\n",
        "        fc_layer_output = self.fc_layer(residual_connection_values) #cross-entrophy does the softmax so we do not do it here again\n",
        "\n",
        "        return fc_layer_output\n",
        "\n",
        "    #code to train the transformer\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=0.1) #passing all weights and biases to Adam, lr 0.1 for this specific model makes it very fast.\n",
        "\n",
        "    def training_step(self, batch, batch_idx): #takes a batch of train data\n",
        "        input_tokens, labels = batch #split data as inputs and labels\n",
        "        output = self.forward(input_tokens[0]) #pass the inputs to the above forward method\n",
        "        loss = self.loss(output, labels[0]) #compare output from transformer to true values\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d4bdd864",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Tokens:\n",
            "\n",
            "\t Great\n",
            "\t Great\n",
            "Predicted Tokens:\n",
            "\n",
            "\t Great\n",
            "\t Great\n",
            "\t <EOS>\n"
          ]
        }
      ],
      "source": [
        "#run training\n",
        "\n",
        "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)\n",
        "\n",
        "model_input = torch.tensor([token_to_id[\"what\"], #create the prompt\n",
        "                            token_to_id[\"is\"],\n",
        "                            token_to_id[\"CNN\"],\n",
        "                            token_to_id[\"<EOS>\"]])\n",
        "\n",
        "input_length = model_input.size(dim=0) #calculate token length\n",
        "\n",
        "predictions = model(model_input) #makes prediction for what token comes next\n",
        "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])]) #-1 -> index the output generated by the <EOS> token\n",
        "#argmax is used to select the largest value form the prediction array\n",
        "predicted_ids = predicted_id\n",
        "\n",
        "max_length = 6\n",
        "for i in range(input_length, max_length): #loop to keep generating the output until max_len or reaches <EOS>\n",
        "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
        "        break\n",
        "    \n",
        "    model_input = torch.cat((model_input, predicted_id)) #Each time we generate a new o/p we add it to i/p so that each predicted is made with full context.\n",
        "    # cat is for concatenation\n",
        "\n",
        "    predictions = model(model_input)\n",
        "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
        "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "    print(\"Predicted Tokens:\\n\")\n",
        "    for id in predicted_ids:\n",
        "        print(\"\\t\", id_to_token[id.item()]) #print generated tokens from converting id numbers to text.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "74767c7d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "ğŸ’¡ Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
            "ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>â”ƒ<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>â”ƒ\n",
              "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>â”‚ we             â”‚ Embedding        â”‚     10 â”‚ train â”‚     0 â”‚\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>â”‚ pe             â”‚ PositionEncoding â”‚      0 â”‚ train â”‚     0 â”‚\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>â”‚ self_attention â”‚ Attention        â”‚     12 â”‚ train â”‚     0 â”‚\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>â”‚ fc_layer       â”‚ Linear           â”‚     15 â”‚ train â”‚     0 â”‚\n",
              "â”‚<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>â”‚ loss           â”‚ CrossEntropyLoss â”‚      0 â”‚ train â”‚     0 â”‚\n",
              "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0mâ”‚ we             â”‚ Embedding        â”‚     10 â”‚ train â”‚     0 â”‚\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0mâ”‚ pe             â”‚ PositionEncoding â”‚      0 â”‚ train â”‚     0 â”‚\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0mâ”‚ self_attention â”‚ Attention        â”‚     12 â”‚ train â”‚     0 â”‚\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0mâ”‚ fc_layer       â”‚ Linear           â”‚     15 â”‚ train â”‚     0 â”‚\n",
              "â”‚\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0mâ”‚ loss           â”‚ CrossEntropyLoss â”‚      0 â”‚ train â”‚     0 â”‚\n",
              "â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 37                                                                                               \n",
              "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total params</span>: 37                                                                                                   \n",
              "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
              "<span style=\"font-weight: bold\">Modules in train mode</span>: 8                                                                                           \n",
              "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
              "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mTrainable params\u001b[0m: 37                                                                                               \n",
              "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal params\u001b[0m: 37                                                                                                   \n",
              "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n",
              "\u001b[1mModules in train mode\u001b[0m: 8                                                                                           \n",
              "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
              "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c7f6b9009714309a325b2653f14a30b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\akshi\\AppData\\Roaming\\Python\\Python313\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:434: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
            "C:\\Users\\akshi\\AppData\\Roaming\\Python\\Python313\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:317: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "    #training the model\n",
        "    trainer = L.Trainer(max_epochs=30)\n",
        "    trainer.fit(model, train_dataloaders=dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eaabb7b",
      "metadata": {},
      "source": [
        "Rerun the model (previous cell ) after training to get better predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c308b90",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
